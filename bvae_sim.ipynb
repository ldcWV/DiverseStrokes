{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce49cd95-e73f-4669-ad61-7a1ba0109052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2658588-69e4-4e04-9a37-96532a68899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simulated_stroke_data/strokes.npy\", \"rb\") as f:\n",
    "    strokes = np.load(f)\n",
    "with open(\"simulated_stroke_data/trajectories.npy\", \"rb\") as f:\n",
    "    trajectories = np.load(f)\n",
    "\n",
    "strokes = torch.from_numpy(strokes).float()\n",
    "trajectories = torch.from_numpy(trajectories).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f490f021-061e-4365-983b-dbe52a1a7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_points(points):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim([-0.3, 1])\n",
    "    plt.ylim([-0.5, 0.5])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    lines = []\n",
    "    for i, (x,y,z) in enumerate(points):\n",
    "        ax.add_patch(plt.Circle((x, y), z/20))\n",
    "        if i != len(points)-1:\n",
    "            lines.append([(x,y),(points[i+1][0], points[i+1][1])])\n",
    "            lc = mc.LineCollection(lines, linewidth=0.1, antialiaseds=False)\n",
    "            ax.add_collection(lc)\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cee8948-8b11-4b4a-8eb9-29f33c8215b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d1c7c55050>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 10\n",
    "_, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(strokes[idx])\n",
    "axs[1].imshow(visualize_points(trajectories[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1822c40-61dc-4fcb-8332-d093d305877c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss=1.6590486764907837, Validation loss=1.1705074310302734\n",
      "Epoch 1: Train loss=0.741226315498352, Validation loss=0.58807772397995\n",
      "Epoch 2: Train loss=0.36009255051612854, Validation loss=0.2538870871067047\n",
      "Epoch 3: Train loss=0.19202235341072083, Validation loss=0.16162635385990143\n",
      "Epoch 4: Train loss=0.14056332409381866, Validation loss=0.11382551491260529\n",
      "Epoch 5: Train loss=0.11666001379489899, Validation loss=0.1000344306230545\n",
      "Epoch 6: Train loss=0.1067449077963829, Validation loss=0.08863767236471176\n",
      "Epoch 7: Train loss=0.09797782450914383, Validation loss=0.089632548391819\n",
      "Epoch 8: Train loss=0.09497073292732239, Validation loss=0.07864965498447418\n",
      "Epoch 9: Train loss=0.09174197167158127, Validation loss=0.07583960890769958\n",
      "Epoch 10: Train loss=0.09007760137319565, Validation loss=0.07536420226097107\n",
      "Epoch 11: Train loss=0.08557751029729843, Validation loss=0.08176679164171219\n",
      "Epoch 12: Train loss=0.08692270517349243, Validation loss=0.07545699924230576\n",
      "Epoch 13: Train loss=0.08495668321847916, Validation loss=0.07563692331314087\n",
      "Epoch 14: Train loss=0.08543660491704941, Validation loss=0.07402779906988144\n",
      "Epoch 15: Train loss=0.08400439471006393, Validation loss=0.07697822898626328\n",
      "Epoch 16: Train loss=0.07853788882493973, Validation loss=0.06806343048810959\n",
      "Epoch 17: Train loss=0.08172156661748886, Validation loss=0.06337475031614304\n",
      "Epoch 18: Train loss=0.0783686563372612, Validation loss=0.0750514417886734\n",
      "Epoch 19: Train loss=0.07723025232553482, Validation loss=0.06639237701892853\n",
      "Epoch 20: Train loss=0.0766599103808403, Validation loss=0.06376660615205765\n",
      "Epoch 21: Train loss=0.07542507350444794, Validation loss=0.07322434335947037\n",
      "Epoch 22: Train loss=0.07396546751260757, Validation loss=0.06580811738967896\n",
      "Epoch 23: Train loss=0.07446689158678055, Validation loss=0.07754122465848923\n",
      "Epoch 24: Train loss=0.07723028212785721, Validation loss=0.06325303018093109\n",
      "Epoch 25: Train loss=0.0735303983092308, Validation loss=0.06605767458677292\n",
      "Epoch 26: Train loss=0.06898286193609238, Validation loss=0.06154724955558777\n",
      "Epoch 27: Train loss=0.07607787847518921, Validation loss=0.06236615404486656\n",
      "Epoch 28: Train loss=0.07340893149375916, Validation loss=0.06232558563351631\n",
      "Epoch 29: Train loss=0.07321510463953018, Validation loss=0.06143839284777641\n",
      "Epoch 30: Train loss=0.07590322941541672, Validation loss=0.06148427724838257\n",
      "Epoch 31: Train loss=0.07128866016864777, Validation loss=0.06305435299873352\n",
      "Epoch 32: Train loss=0.07021737098693848, Validation loss=0.056450583040714264\n",
      "Epoch 33: Train loss=0.07070702314376831, Validation loss=0.06179394945502281\n",
      "Epoch 34: Train loss=0.06844386458396912, Validation loss=0.06802884489297867\n",
      "Epoch 35: Train loss=0.07024575769901276, Validation loss=0.05119850113987923\n",
      "Epoch 36: Train loss=0.06928419321775436, Validation loss=0.057207949459552765\n",
      "Epoch 37: Train loss=0.07309116423130035, Validation loss=0.051513671875\n",
      "Epoch 38: Train loss=0.06378724426031113, Validation loss=0.06525006890296936\n",
      "Epoch 39: Train loss=0.06741105020046234, Validation loss=0.057920511811971664\n",
      "Epoch 40: Train loss=0.06857434660196304, Validation loss=0.059902485460042953\n",
      "Epoch 41: Train loss=0.06802275776863098, Validation loss=0.05799270421266556\n",
      "Epoch 42: Train loss=0.06672360748052597, Validation loss=0.05750754475593567\n",
      "Epoch 43: Train loss=0.06721179932355881, Validation loss=0.055817391723394394\n",
      "Epoch 44: Train loss=0.06421276181936264, Validation loss=0.06323602795600891\n",
      "Epoch 45: Train loss=0.06580998748540878, Validation loss=0.06420837342739105\n",
      "Epoch 46: Train loss=0.06740257889032364, Validation loss=0.05755650997161865\n",
      "Epoch 47: Train loss=0.06094559654593468, Validation loss=0.05343163013458252\n",
      "Epoch 48: Train loss=0.06280326098203659, Validation loss=0.055253561586141586\n",
      "Epoch 49: Train loss=0.0602707639336586, Validation loss=0.05976347252726555\n",
      "Epoch 50: Train loss=0.06183812767267227, Validation loss=0.05667676404118538\n",
      "Epoch 51: Train loss=0.06118887662887573, Validation loss=0.06484711170196533\n",
      "Epoch 52: Train loss=0.06056472286581993, Validation loss=0.05416987091302872\n",
      "Epoch 53: Train loss=0.06482953578233719, Validation loss=0.05665700510144234\n",
      "Epoch 54: Train loss=0.06334814429283142, Validation loss=0.05105689913034439\n",
      "Epoch 55: Train loss=0.05708843469619751, Validation loss=0.0654587373137474\n",
      "Epoch 56: Train loss=0.05710005387663841, Validation loss=0.053037066012620926\n",
      "Epoch 57: Train loss=0.05833287164568901, Validation loss=0.056876134127378464\n",
      "Epoch 58: Train loss=0.06714539229869843, Validation loss=0.049304526299238205\n",
      "Epoch 59: Train loss=0.05531247332692146, Validation loss=0.05537164956331253\n",
      "Epoch 60: Train loss=0.06180129200220108, Validation loss=0.04727187380194664\n",
      "Epoch 61: Train loss=0.05914483219385147, Validation loss=0.05261112377047539\n",
      "Epoch 62: Train loss=0.06193437799811363, Validation loss=0.056884851306676865\n",
      "Epoch 63: Train loss=0.06600479036569595, Validation loss=0.04842877388000488\n",
      "Epoch 64: Train loss=0.057294346392154694, Validation loss=0.05457458645105362\n",
      "Epoch 65: Train loss=0.0564020536839962, Validation loss=0.04844462871551514\n",
      "Epoch 66: Train loss=0.0555824413895607, Validation loss=0.04944635555148125\n",
      "Epoch 67: Train loss=0.05582653358578682, Validation loss=0.047124847769737244\n",
      "Epoch 68: Train loss=0.06083167716860771, Validation loss=0.0557718351483345\n",
      "Epoch 69: Train loss=0.06303755939006805, Validation loss=0.05455981567502022\n",
      "Epoch 70: Train loss=0.06063082441687584, Validation loss=0.045030687004327774\n",
      "Epoch 71: Train loss=0.05918125435709953, Validation loss=0.05961005762219429\n",
      "Epoch 72: Train loss=0.05545886978507042, Validation loss=0.044132012873888016\n",
      "Epoch 73: Train loss=0.05945345386862755, Validation loss=0.042949970811605453\n",
      "Epoch 74: Train loss=0.05278156325221062, Validation loss=0.046958353370428085\n",
      "Epoch 75: Train loss=0.05723956972360611, Validation loss=0.0504005141556263\n",
      "Epoch 76: Train loss=0.05368269607424736, Validation loss=0.04922645911574364\n",
      "Epoch 77: Train loss=0.050917454063892365, Validation loss=0.04081123694777489\n",
      "Epoch 78: Train loss=0.05074096843600273, Validation loss=0.04824061691761017\n",
      "Epoch 79: Train loss=0.05136333033442497, Validation loss=0.050189025700092316\n",
      "Epoch 80: Train loss=0.05297417566180229, Validation loss=0.05692499876022339\n",
      "Epoch 81: Train loss=0.051852088421583176, Validation loss=0.05572275444865227\n",
      "Epoch 82: Train loss=0.05139387771487236, Validation loss=0.05252636596560478\n",
      "Epoch 83: Train loss=0.050099171698093414, Validation loss=0.0393846370279789\n",
      "Epoch 84: Train loss=0.05137334018945694, Validation loss=0.04185206815600395\n",
      "Epoch 85: Train loss=0.045744333416223526, Validation loss=0.045930247753858566\n",
      "Epoch 86: Train loss=0.04832499474287033, Validation loss=0.06093142554163933\n",
      "Epoch 87: Train loss=0.05080018192529678, Validation loss=0.04587283357977867\n",
      "Epoch 88: Train loss=0.05082432180643082, Validation loss=0.04372919350862503\n",
      "Epoch 89: Train loss=0.04806899651885033, Validation loss=0.04834849387407303\n",
      "Epoch 90: Train loss=0.046752315014600754, Validation loss=0.046838123351335526\n",
      "Epoch 91: Train loss=0.04733453691005707, Validation loss=0.044410623610019684\n",
      "Epoch 92: Train loss=0.05026547983288765, Validation loss=0.05472620204091072\n",
      "Epoch 93: Train loss=0.052017468959093094, Validation loss=0.043299417942762375\n",
      "Epoch 94: Train loss=0.05038600042462349, Validation loss=0.04992181807756424\n",
      "Epoch 95: Train loss=0.0450742244720459, Validation loss=0.04047321900725365\n",
      "Epoch 96: Train loss=0.04921397194266319, Validation loss=0.041097451001405716\n",
      "Epoch 97: Train loss=0.04845479130744934, Validation loss=0.04489515349268913\n",
      "Epoch 98: Train loss=0.04684062674641609, Validation loss=0.04397968575358391\n",
      "Epoch 99: Train loss=0.04427449777722359, Validation loss=0.04572899267077446\n",
      "Epoch 100: Train loss=0.04806990548968315, Validation loss=0.04417569935321808\n",
      "Epoch 101: Train loss=0.04597215726971626, Validation loss=0.04000125452876091\n",
      "Epoch 102: Train loss=0.0472923219203949, Validation loss=0.05578240752220154\n",
      "Epoch 103: Train loss=0.0446479469537735, Validation loss=0.059071242809295654\n",
      "Epoch 104: Train loss=0.04694652184844017, Validation loss=0.050825245678424835\n",
      "Epoch 105: Train loss=0.04688393324613571, Validation loss=0.05046326667070389\n",
      "Epoch 106: Train loss=0.0442909374833107, Validation loss=0.044494591653347015\n",
      "Epoch 107: Train loss=0.04455377906560898, Validation loss=0.040601588785648346\n",
      "Epoch 108: Train loss=0.04295409098267555, Validation loss=0.041222091764211655\n",
      "Epoch 109: Train loss=0.044054679572582245, Validation loss=0.04280189052224159\n",
      "Epoch 110: Train loss=0.04531064257025719, Validation loss=0.04352051764726639\n",
      "Epoch 111: Train loss=0.04536941275000572, Validation loss=0.047575101256370544\n",
      "Epoch 112: Train loss=0.043609168380498886, Validation loss=0.04440208151936531\n",
      "Epoch 113: Train loss=0.043537747114896774, Validation loss=0.04026665911078453\n",
      "Epoch 114: Train loss=0.04505963250994682, Validation loss=0.04312216863036156\n",
      "Epoch 115: Train loss=0.041147373616695404, Validation loss=0.04533570632338524\n",
      "Epoch 116: Train loss=0.043538663536310196, Validation loss=0.04313711076974869\n",
      "Epoch 117: Train loss=0.0432400107383728, Validation loss=0.04293898865580559\n",
      "Epoch 118: Train loss=0.04392797872424126, Validation loss=0.03814445808529854\n",
      "Epoch 119: Train loss=0.04338119551539421, Validation loss=0.03785720095038414\n",
      "Epoch 120: Train loss=0.04515501484274864, Validation loss=0.043322402983903885\n",
      "Epoch 121: Train loss=0.04334009066224098, Validation loss=0.036232467740774155\n",
      "Epoch 122: Train loss=0.04114934056997299, Validation loss=0.04115605726838112\n",
      "Epoch 123: Train loss=0.04266735538840294, Validation loss=0.0330420583486557\n",
      "Epoch 124: Train loss=0.04216092452406883, Validation loss=0.04204405099153519\n",
      "Epoch 125: Train loss=0.040939804166555405, Validation loss=0.04015539214015007\n",
      "Epoch 126: Train loss=0.040305912494659424, Validation loss=0.04748758301138878\n",
      "Epoch 127: Train loss=0.041883330792188644, Validation loss=0.03442555293440819\n",
      "Epoch 128: Train loss=0.04251633211970329, Validation loss=0.03845162317156792\n",
      "Epoch 129: Train loss=0.042784739285707474, Validation loss=0.03681492805480957\n",
      "Epoch 130: Train loss=0.03939943388104439, Validation loss=0.041987158358097076\n",
      "Epoch 131: Train loss=0.03768809512257576, Validation loss=0.03543320298194885\n",
      "Epoch 132: Train loss=0.040364958345890045, Validation loss=0.04350733384490013\n",
      "Epoch 133: Train loss=0.039874717593193054, Validation loss=0.035225726664066315\n",
      "Epoch 134: Train loss=0.04020882025361061, Validation loss=0.04057493060827255\n",
      "Epoch 135: Train loss=0.042729541659355164, Validation loss=0.03780870512127876\n",
      "Epoch 136: Train loss=0.040627576410770416, Validation loss=0.038989558815956116\n",
      "Epoch 137: Train loss=0.040209509432315826, Validation loss=0.04130140691995621\n",
      "Epoch 138: Train loss=0.04132719337940216, Validation loss=0.04013480618596077\n",
      "Epoch 139: Train loss=0.03901313617825508, Validation loss=0.0450042299926281\n",
      "Epoch 140: Train loss=0.03683265671133995, Validation loss=0.037968602031469345\n",
      "Epoch 141: Train loss=0.0378866121172905, Validation loss=0.04772639274597168\n",
      "Epoch 142: Train loss=0.041318170726299286, Validation loss=0.03777848556637764\n",
      "Epoch 143: Train loss=0.039445552974939346, Validation loss=0.04900605231523514\n",
      "Epoch 144: Train loss=0.03687227889895439, Validation loss=0.04119120165705681\n",
      "Epoch 145: Train loss=0.036443814635276794, Validation loss=0.043963223695755005\n",
      "Epoch 146: Train loss=0.03713339939713478, Validation loss=0.040756646543741226\n",
      "Epoch 147: Train loss=0.03512636199593544, Validation loss=0.03774488344788551\n",
      "Epoch 148: Train loss=0.03588612750172615, Validation loss=0.03538232669234276\n",
      "Epoch 149: Train loss=0.03740859776735306, Validation loss=0.044415563344955444\n",
      "Epoch 150: Train loss=0.03562086820602417, Validation loss=0.03902329131960869\n",
      "Epoch 151: Train loss=0.03700617700815201, Validation loss=0.03543348237872124\n",
      "Epoch 152: Train loss=0.037514280527830124, Validation loss=0.03736856207251549\n",
      "Epoch 153: Train loss=0.034251030534505844, Validation loss=0.036057982593774796\n",
      "Epoch 154: Train loss=0.03479284048080444, Validation loss=0.03080740012228489\n",
      "Epoch 155: Train loss=0.036720942705869675, Validation loss=0.033240269869565964\n",
      "Epoch 156: Train loss=0.034618526697158813, Validation loss=0.03159993886947632\n",
      "Epoch 157: Train loss=0.037091825157403946, Validation loss=0.03494785353541374\n",
      "Epoch 158: Train loss=0.03234020993113518, Validation loss=0.037587009370326996\n",
      "Epoch 159: Train loss=0.03392649069428444, Validation loss=0.03810052573680878\n",
      "Epoch 160: Train loss=0.035639628767967224, Validation loss=0.042742904275655746\n",
      "Epoch 161: Train loss=0.03503675386309624, Validation loss=0.04332657903432846\n",
      "Epoch 162: Train loss=0.03451869264245033, Validation loss=0.03271173685789108\n",
      "Epoch 163: Train loss=0.033698078244924545, Validation loss=0.035529982298612595\n",
      "Epoch 164: Train loss=0.0340791754424572, Validation loss=0.029150670394301414\n",
      "Epoch 165: Train loss=0.03588182106614113, Validation loss=0.0383574478328228\n",
      "Epoch 166: Train loss=0.033897969871759415, Validation loss=0.03313704952597618\n",
      "Epoch 167: Train loss=0.03399289771914482, Validation loss=0.03125350922346115\n",
      "Epoch 168: Train loss=0.035121720284223557, Validation loss=0.04182738810777664\n",
      "Epoch 169: Train loss=0.03498313948512077, Validation loss=0.031028782948851585\n",
      "Epoch 170: Train loss=0.034861501306295395, Validation loss=0.038212019950151443\n",
      "Epoch 171: Train loss=0.035480208694934845, Validation loss=0.046059608459472656\n",
      "Epoch 172: Train loss=0.03690921515226364, Validation loss=0.029005257412791252\n",
      "Epoch 173: Train loss=0.03561211749911308, Validation loss=0.027726123109459877\n",
      "Epoch 174: Train loss=0.03555509075522423, Validation loss=0.03600220009684563\n",
      "Epoch 175: Train loss=0.03617781400680542, Validation loss=0.03250839188694954\n",
      "Epoch 176: Train loss=0.031211242079734802, Validation loss=0.030636439099907875\n",
      "Epoch 177: Train loss=0.034717660397291183, Validation loss=0.03006214275956154\n",
      "Epoch 178: Train loss=0.03466422110795975, Validation loss=0.04597419500350952\n",
      "Epoch 179: Train loss=0.036099910736083984, Validation loss=0.04235514998435974\n",
      "Epoch 180: Train loss=0.034726571291685104, Validation loss=0.04126663878560066\n",
      "Epoch 181: Train loss=0.03252590820193291, Validation loss=0.03272892162203789\n",
      "Epoch 182: Train loss=0.03269316256046295, Validation loss=0.033355630934238434\n",
      "Epoch 183: Train loss=0.03260594233870506, Validation loss=0.03589700907468796\n",
      "Epoch 184: Train loss=0.030303260311484337, Validation loss=0.032401155680418015\n",
      "Epoch 185: Train loss=0.032544154673814774, Validation loss=0.027110407128930092\n",
      "Epoch 186: Train loss=0.03281025215983391, Validation loss=0.042890917509794235\n",
      "Epoch 187: Train loss=0.03373536095023155, Validation loss=0.03365273401141167\n",
      "Epoch 188: Train loss=0.03150954470038414, Validation loss=0.03259257599711418\n",
      "Epoch 189: Train loss=0.031169136986136436, Validation loss=0.03217210993170738\n",
      "Epoch 190: Train loss=0.030514081940054893, Validation loss=0.03359026834368706\n",
      "Epoch 191: Train loss=0.03130019083619118, Validation loss=0.03643902763724327\n",
      "Epoch 192: Train loss=0.03149503096938133, Validation loss=0.044467926025390625\n",
      "Epoch 193: Train loss=0.03537682071328163, Validation loss=0.03639104589819908\n",
      "Epoch 194: Train loss=0.035756636410951614, Validation loss=0.04226059839129448\n",
      "Epoch 195: Train loss=0.032057370990514755, Validation loss=0.03216535225510597\n",
      "Epoch 196: Train loss=0.029963111504912376, Validation loss=0.03200692683458328\n",
      "Epoch 197: Train loss=0.030964672565460205, Validation loss=0.0384039580821991\n",
      "Epoch 198: Train loss=0.02973981574177742, Validation loss=0.027569500729441643\n",
      "Epoch 199: Train loss=0.02943749725818634, Validation loss=0.02955774776637554\n",
      "Epoch 200: Train loss=0.031209751963615417, Validation loss=0.03029315546154976\n",
      "Epoch 201: Train loss=0.029202694073319435, Validation loss=0.02617533877491951\n",
      "Epoch 202: Train loss=0.028467794880270958, Validation loss=0.03453939035534859\n",
      "Epoch 203: Train loss=0.031243210658431053, Validation loss=0.030990267172455788\n",
      "Epoch 204: Train loss=0.031404223293066025, Validation loss=0.029392052441835403\n",
      "Epoch 205: Train loss=0.03095266781747341, Validation loss=0.04080081731081009\n",
      "Epoch 206: Train loss=0.028248265385627747, Validation loss=0.035696931183338165\n",
      "Epoch 207: Train loss=0.027368146926164627, Validation loss=0.033145103603601456\n",
      "Epoch 208: Train loss=0.02814539335668087, Validation loss=0.041473139077425\n",
      "Epoch 209: Train loss=0.028937919065356255, Validation loss=0.032845254987478256\n",
      "Epoch 210: Train loss=0.0271686352789402, Validation loss=0.061285924166440964\n",
      "Epoch 211: Train loss=0.028230920433998108, Validation loss=0.032920144498348236\n",
      "Epoch 212: Train loss=0.024516429752111435, Validation loss=0.028878619894385338\n",
      "Epoch 213: Train loss=0.026357829570770264, Validation loss=0.04260195419192314\n",
      "Epoch 214: Train loss=0.028451988473534584, Validation loss=0.02870303951203823\n",
      "Epoch 215: Train loss=0.028356194496154785, Validation loss=0.03588753938674927\n",
      "Epoch 216: Train loss=0.02619231306016445, Validation loss=0.03381136432290077\n",
      "Epoch 217: Train loss=0.025154396891593933, Validation loss=0.032780006527900696\n",
      "Epoch 218: Train loss=0.025662416592240334, Validation loss=0.035705696791410446\n",
      "Epoch 219: Train loss=0.02601621486246586, Validation loss=0.02993100695312023\n",
      "Epoch 220: Train loss=0.026399021968245506, Validation loss=0.03423883020877838\n",
      "Epoch 221: Train loss=0.02655036747455597, Validation loss=0.036969996988773346\n",
      "Epoch 222: Train loss=0.02734409086406231, Validation loss=0.025690263137221336\n",
      "Epoch 223: Train loss=0.026749759912490845, Validation loss=0.029478270560503006\n",
      "Epoch 224: Train loss=0.027725879102945328, Validation loss=0.028123734518885612\n",
      "Epoch 225: Train loss=0.02717660181224346, Validation loss=0.034341562539339066\n",
      "Epoch 226: Train loss=0.02739054523408413, Validation loss=0.028547966852784157\n",
      "Epoch 227: Train loss=0.024935675784945488, Validation loss=0.027879098430275917\n",
      "Epoch 228: Train loss=0.025818010792136192, Validation loss=0.02407919242978096\n",
      "Epoch 229: Train loss=0.02613186277449131, Validation loss=0.027159521356225014\n",
      "Epoch 230: Train loss=0.02575666271150112, Validation loss=0.03406856209039688\n",
      "Epoch 231: Train loss=0.024893198162317276, Validation loss=0.03537273034453392\n",
      "Epoch 232: Train loss=0.02918839454650879, Validation loss=0.02696075476706028\n",
      "Epoch 233: Train loss=0.026709184050559998, Validation loss=0.028183041140437126\n",
      "Epoch 234: Train loss=0.027223696932196617, Validation loss=0.022394556552171707\n",
      "Epoch 235: Train loss=0.024061137810349464, Validation loss=0.026244178414344788\n",
      "Epoch 236: Train loss=0.02540324069559574, Validation loss=0.030984556302428246\n",
      "Epoch 237: Train loss=0.025590956211090088, Validation loss=0.025675464421510696\n",
      "Epoch 238: Train loss=0.02577279321849346, Validation loss=0.03313979133963585\n",
      "Epoch 239: Train loss=0.02481197752058506, Validation loss=0.026602599769830704\n",
      "Epoch 240: Train loss=0.02715725265443325, Validation loss=0.04222673922777176\n",
      "Epoch 241: Train loss=0.024797597900032997, Validation loss=0.041969310492277145\n",
      "Epoch 242: Train loss=0.024273080751299858, Validation loss=0.034513331949710846\n",
      "Epoch 243: Train loss=0.025280317291617393, Validation loss=0.026243632659316063\n",
      "Epoch 244: Train loss=0.024113265797495842, Validation loss=0.026841197162866592\n",
      "Epoch 245: Train loss=0.023011231794953346, Validation loss=0.03256625682115555\n",
      "Epoch 246: Train loss=0.02393483929336071, Validation loss=0.036925602704286575\n",
      "Epoch 247: Train loss=0.02516346052289009, Validation loss=0.0250242929905653\n",
      "Epoch 248: Train loss=0.025684893131256104, Validation loss=0.030044863000512123\n",
      "Epoch 249: Train loss=0.024626027792692184, Validation loss=0.021757064387202263\n",
      "Epoch 250: Train loss=0.022416269406676292, Validation loss=0.0237348023802042\n",
      "Epoch 251: Train loss=0.023088039830327034, Validation loss=0.03402494266629219\n",
      "Epoch 252: Train loss=0.022756211459636688, Validation loss=0.03407592326402664\n",
      "Epoch 253: Train loss=0.021907087415456772, Validation loss=0.024978265166282654\n",
      "Epoch 254: Train loss=0.023660538718104362, Validation loss=0.028195688501000404\n",
      "Epoch 255: Train loss=0.02198190428316593, Validation loss=0.03615061938762665\n",
      "Epoch 256: Train loss=0.024306975305080414, Validation loss=0.03521113842725754\n",
      "Epoch 257: Train loss=0.024254756048321724, Validation loss=0.02778860367834568\n",
      "Epoch 258: Train loss=0.02340293489396572, Validation loss=0.03463485464453697\n",
      "Epoch 259: Train loss=0.021941378712654114, Validation loss=0.02719733491539955\n",
      "Epoch 260: Train loss=0.02678893506526947, Validation loss=0.029793282970786095\n",
      "Epoch 261: Train loss=0.02326813153922558, Validation loss=0.0313311405479908\n",
      "Epoch 262: Train loss=0.022797565907239914, Validation loss=0.03003082610666752\n",
      "Epoch 263: Train loss=0.023216217756271362, Validation loss=0.026776328682899475\n",
      "Epoch 264: Train loss=0.02193734608590603, Validation loss=0.022138407453894615\n",
      "Epoch 265: Train loss=0.023330537602305412, Validation loss=0.02001682110130787\n",
      "Epoch 266: Train loss=0.022014493122696877, Validation loss=0.02911568619310856\n",
      "Epoch 267: Train loss=0.02151285484433174, Validation loss=0.026039034128189087\n",
      "Epoch 268: Train loss=0.022722141817212105, Validation loss=0.021404119208455086\n",
      "Epoch 269: Train loss=0.0233177300542593, Validation loss=0.035505518317222595\n",
      "Epoch 270: Train loss=0.023551365360617638, Validation loss=0.0363999642431736\n",
      "Epoch 271: Train loss=0.022947944700717926, Validation loss=0.039576899260282516\n",
      "Epoch 272: Train loss=0.020668087527155876, Validation loss=0.02284744195640087\n",
      "Epoch 273: Train loss=0.019796222448349, Validation loss=0.03010445646941662\n",
      "Epoch 274: Train loss=0.022130658850073814, Validation loss=0.03459818661212921\n",
      "Epoch 275: Train loss=0.02159266360104084, Validation loss=0.056349098682403564\n",
      "Epoch 276: Train loss=0.020163757726550102, Validation loss=0.026489486917853355\n",
      "Epoch 277: Train loss=0.02073841728270054, Validation loss=0.030701858922839165\n",
      "Epoch 278: Train loss=0.021811632439494133, Validation loss=0.027582509443163872\n",
      "Epoch 279: Train loss=0.021910730749368668, Validation loss=0.020693941041827202\n",
      "Epoch 280: Train loss=0.0243527889251709, Validation loss=0.021977026015520096\n",
      "Epoch 281: Train loss=0.020860861986875534, Validation loss=0.040745317935943604\n",
      "Epoch 282: Train loss=0.02000444196164608, Validation loss=0.04085049033164978\n",
      "Epoch 283: Train loss=0.020448988303542137, Validation loss=0.023374129086732864\n",
      "Epoch 284: Train loss=0.02212045155465603, Validation loss=0.02944844402372837\n",
      "Epoch 285: Train loss=0.02042504958808422, Validation loss=0.030218729749321938\n",
      "Epoch 286: Train loss=0.019628046080470085, Validation loss=0.02047746442258358\n",
      "Epoch 287: Train loss=0.023009179159998894, Validation loss=0.025605982169508934\n",
      "Epoch 288: Train loss=0.021705443039536476, Validation loss=0.026751453056931496\n",
      "Epoch 289: Train loss=0.02137172967195511, Validation loss=0.028453653678297997\n",
      "Epoch 290: Train loss=0.01906616799533367, Validation loss=0.039276182651519775\n",
      "Epoch 291: Train loss=0.018311338499188423, Validation loss=0.01892680488526821\n",
      "Epoch 292: Train loss=0.019133001565933228, Validation loss=0.029389536008238792\n",
      "Epoch 293: Train loss=0.019678190350532532, Validation loss=0.031790316104888916\n",
      "Epoch 294: Train loss=0.020575376227498055, Validation loss=0.04116946831345558\n",
      "Epoch 295: Train loss=0.020632987841963768, Validation loss=0.03077327087521553\n",
      "Epoch 296: Train loss=0.0197764839977026, Validation loss=0.02233233116567135\n",
      "Epoch 297: Train loss=0.021130096167325974, Validation loss=0.019805407151579857\n",
      "Epoch 298: Train loss=0.022645985707640648, Validation loss=0.03121930919587612\n",
      "Epoch 299: Train loss=0.021535512059926987, Validation loss=0.021492211148142815\n",
      "Epoch 300: Train loss=0.018453914672136307, Validation loss=0.025830646976828575\n",
      "Epoch 301: Train loss=0.02176305092871189, Validation loss=0.024273961782455444\n",
      "Epoch 302: Train loss=0.021488329395651817, Validation loss=0.045089613646268845\n",
      "Epoch 303: Train loss=0.020573431625962257, Validation loss=0.05278141424059868\n",
      "Epoch 304: Train loss=0.019566308706998825, Validation loss=0.036423902958631516\n",
      "Epoch 305: Train loss=0.017962755635380745, Validation loss=0.03236619755625725\n",
      "Epoch 306: Train loss=0.018284006044268608, Validation loss=0.02413240633904934\n",
      "Epoch 307: Train loss=0.018801579251885414, Validation loss=0.022458568215370178\n",
      "Epoch 308: Train loss=0.0190946813672781, Validation loss=0.02259998209774494\n",
      "Epoch 309: Train loss=0.019141875207424164, Validation loss=0.032907821238040924\n",
      "Epoch 310: Train loss=0.018269576132297516, Validation loss=0.03429730609059334\n",
      "Epoch 311: Train loss=0.018375393003225327, Validation loss=0.03080269694328308\n",
      "Epoch 312: Train loss=0.019399648532271385, Validation loss=0.02853180468082428\n",
      "Epoch 313: Train loss=0.018731167539954185, Validation loss=0.02436501905322075\n",
      "Epoch 314: Train loss=0.018819158896803856, Validation loss=0.021042153239250183\n",
      "Epoch 315: Train loss=0.01969471015036106, Validation loss=0.03454951196908951\n",
      "Epoch 316: Train loss=0.020669041201472282, Validation loss=0.05826975777745247\n",
      "Epoch 317: Train loss=0.01934470422565937, Validation loss=0.025884244590997696\n",
      "Epoch 318: Train loss=0.017603885382413864, Validation loss=0.025074303150177002\n",
      "Epoch 319: Train loss=0.019698455929756165, Validation loss=0.029361678287386894\n",
      "Epoch 320: Train loss=0.019036665558815002, Validation loss=0.03751155361533165\n",
      "Epoch 321: Train loss=0.018472889438271523, Validation loss=0.03774406388401985\n",
      "Epoch 322: Train loss=0.018778206780552864, Validation loss=0.02185610868036747\n",
      "Epoch 323: Train loss=0.018001368269324303, Validation loss=0.02387414686381817\n",
      "Epoch 324: Train loss=0.017196061089634895, Validation loss=0.028545839712023735\n",
      "Epoch 325: Train loss=0.01947452686727047, Validation loss=0.02725052833557129\n",
      "Epoch 326: Train loss=0.018044399097561836, Validation loss=0.03020620346069336\n",
      "Epoch 327: Train loss=0.018835950642824173, Validation loss=0.02238636463880539\n",
      "Epoch 328: Train loss=0.01999010518193245, Validation loss=0.026692494750022888\n",
      "Epoch 329: Train loss=0.019713599234819412, Validation loss=0.025797460228204727\n",
      "Epoch 330: Train loss=0.018341064453125, Validation loss=0.02258961834013462\n",
      "Epoch 331: Train loss=0.01863439939916134, Validation loss=0.034281615167856216\n",
      "Epoch 332: Train loss=0.02015230804681778, Validation loss=0.021492987871170044\n",
      "Epoch 333: Train loss=0.018269702792167664, Validation loss=0.02338111214339733\n",
      "Epoch 334: Train loss=0.016395071521401405, Validation loss=0.01978331245481968\n",
      "Epoch 335: Train loss=0.018007194623351097, Validation loss=0.03074515424668789\n",
      "Epoch 336: Train loss=0.018441518768668175, Validation loss=0.027734888717532158\n",
      "Epoch 337: Train loss=0.018222620710730553, Validation loss=0.026151275262236595\n",
      "Epoch 338: Train loss=0.01915154419839382, Validation loss=0.028207197785377502\n",
      "Epoch 339: Train loss=0.015463319607079029, Validation loss=0.043844666332006454\n",
      "Epoch 340: Train loss=0.01625225506722927, Validation loss=0.02861446514725685\n",
      "Epoch 341: Train loss=0.014989609830081463, Validation loss=0.033505816012620926\n",
      "Epoch 342: Train loss=0.018536752089858055, Validation loss=0.01676105707883835\n",
      "Epoch 343: Train loss=0.01649678684771061, Validation loss=0.021769938990473747\n",
      "Epoch 344: Train loss=0.017107877880334854, Validation loss=0.02995959483087063\n",
      "Epoch 345: Train loss=0.016816547140479088, Validation loss=0.03519925847649574\n",
      "Epoch 346: Train loss=0.01703297160565853, Validation loss=0.02371458150446415\n",
      "Epoch 347: Train loss=0.01829141192138195, Validation loss=0.030428046360611916\n",
      "Epoch 348: Train loss=0.01891581527888775, Validation loss=0.014135702513158321\n",
      "Epoch 349: Train loss=0.016872813925147057, Validation loss=0.029906395822763443\n",
      "Epoch 350: Train loss=0.014723123982548714, Validation loss=0.022674569860100746\n",
      "Epoch 351: Train loss=0.015458427369594574, Validation loss=0.03935972973704338\n",
      "Epoch 352: Train loss=0.016381343826651573, Validation loss=0.03719070181250572\n",
      "Epoch 353: Train loss=0.015917155891656876, Validation loss=0.04203522950410843\n",
      "Epoch 354: Train loss=0.015444074757397175, Validation loss=0.019549844786524773\n",
      "Epoch 355: Train loss=0.019786374643445015, Validation loss=0.030111348256468773\n",
      "Epoch 356: Train loss=0.01707303524017334, Validation loss=0.021707704290747643\n",
      "Epoch 357: Train loss=0.01815023459494114, Validation loss=0.015067274682223797\n",
      "Epoch 358: Train loss=0.017334220930933952, Validation loss=0.03853156417608261\n",
      "Epoch 359: Train loss=0.01607563905417919, Validation loss=0.02693013846874237\n",
      "Epoch 360: Train loss=0.016833391040563583, Validation loss=0.025846725329756737\n",
      "Epoch 361: Train loss=0.016473783180117607, Validation loss=0.025840533897280693\n",
      "Epoch 362: Train loss=0.015658915042877197, Validation loss=0.027972055599093437\n",
      "Epoch 363: Train loss=0.015753131359815598, Validation loss=0.021478990092873573\n",
      "Epoch 364: Train loss=0.016165442764759064, Validation loss=0.028430817648768425\n",
      "Epoch 365: Train loss=0.016587048768997192, Validation loss=0.033194005489349365\n",
      "Epoch 366: Train loss=0.013809709809720516, Validation loss=0.033210691064596176\n",
      "Epoch 367: Train loss=0.01545591652393341, Validation loss=0.02974582277238369\n",
      "Epoch 368: Train loss=0.018015574663877487, Validation loss=0.02484244666993618\n",
      "Epoch 369: Train loss=0.016160523518919945, Validation loss=0.019945219159126282\n",
      "Epoch 370: Train loss=0.015414289198815823, Validation loss=0.0148068368434906\n",
      "Epoch 371: Train loss=0.016733935102820396, Validation loss=0.02617715112864971\n",
      "Epoch 372: Train loss=0.017636876553297043, Validation loss=0.023330239579081535\n",
      "Epoch 373: Train loss=0.017124705016613007, Validation loss=0.026880566030740738\n",
      "Epoch 374: Train loss=0.016593173146247864, Validation loss=0.0273843165487051\n",
      "Epoch 375: Train loss=0.016737623140215874, Validation loss=0.026432553306221962\n",
      "Epoch 376: Train loss=0.017515309154987335, Validation loss=0.03250466659665108\n",
      "Epoch 377: Train loss=0.014560195617377758, Validation loss=0.03475620225071907\n",
      "Epoch 378: Train loss=0.015493735671043396, Validation loss=0.0407470241189003\n",
      "Epoch 379: Train loss=0.016742831096053123, Validation loss=0.06153206154704094\n",
      "Epoch 380: Train loss=0.017130102962255478, Validation loss=0.015824437141418457\n",
      "Epoch 381: Train loss=0.018063968047499657, Validation loss=0.026332521811127663\n",
      "Epoch 382: Train loss=0.015699906274676323, Validation loss=0.024075206369161606\n",
      "Epoch 383: Train loss=0.015091213397681713, Validation loss=0.026095712557435036\n",
      "Epoch 384: Train loss=0.01686057075858116, Validation loss=0.0205641221255064\n",
      "Epoch 385: Train loss=0.015294176526367664, Validation loss=0.025752410292625427\n",
      "Epoch 386: Train loss=0.014510798268020153, Validation loss=0.016705026850104332\n",
      "Epoch 387: Train loss=0.014152399264276028, Validation loss=0.023067167028784752\n",
      "Epoch 388: Train loss=0.013965393416583538, Validation loss=0.044564224779605865\n",
      "Epoch 389: Train loss=0.014971205964684486, Validation loss=0.03386252745985985\n",
      "Epoch 390: Train loss=0.015112663619220257, Validation loss=0.05231337621808052\n",
      "Epoch 391: Train loss=0.013741343282163143, Validation loss=0.019373541697859764\n",
      "Epoch 392: Train loss=0.013086865656077862, Validation loss=0.017931759357452393\n",
      "Epoch 393: Train loss=0.013527698814868927, Validation loss=0.06126575544476509\n",
      "Epoch 394: Train loss=0.01384378969669342, Validation loss=0.038897573947906494\n",
      "Epoch 395: Train loss=0.015447394922375679, Validation loss=0.014227290637791157\n",
      "Epoch 396: Train loss=0.013662146404385567, Validation loss=0.021638469770550728\n",
      "Epoch 397: Train loss=0.01294891070574522, Validation loss=0.02483319491147995\n",
      "Epoch 398: Train loss=0.015036028809845448, Validation loss=0.024538083001971245\n",
      "Epoch 399: Train loss=0.016258900985121727, Validation loss=0.024898124858736992\n",
      "Epoch 400: Train loss=0.01567903719842434, Validation loss=0.016990192234516144\n",
      "Epoch 401: Train loss=0.014637276530265808, Validation loss=0.02005487121641636\n",
      "Epoch 402: Train loss=0.014231659471988678, Validation loss=0.04733973741531372\n",
      "Epoch 403: Train loss=0.013307391665875912, Validation loss=0.01658404991030693\n",
      "Epoch 404: Train loss=0.015624696388840675, Validation loss=0.016295289620757103\n",
      "Epoch 405: Train loss=0.014372340403497219, Validation loss=0.07735162228345871\n",
      "Epoch 406: Train loss=0.014639908447861671, Validation loss=0.019554272294044495\n",
      "Epoch 407: Train loss=0.014878349378705025, Validation loss=0.01909548044204712\n",
      "Epoch 408: Train loss=0.015143999829888344, Validation loss=0.033541928976774216\n",
      "Epoch 409: Train loss=0.014079095795750618, Validation loss=0.015713438391685486\n",
      "Epoch 410: Train loss=0.012995167635381222, Validation loss=0.02673405408859253\n",
      "Epoch 411: Train loss=0.01400651503354311, Validation loss=0.020613867789506912\n",
      "Epoch 412: Train loss=0.013542041182518005, Validation loss=0.02033432386815548\n",
      "Epoch 413: Train loss=0.012801909819245338, Validation loss=0.015099771320819855\n",
      "Epoch 414: Train loss=0.014090994372963905, Validation loss=0.017547843977808952\n",
      "Epoch 415: Train loss=0.013221122324466705, Validation loss=0.013527336530387402\n",
      "Epoch 416: Train loss=0.01617896929383278, Validation loss=0.04429022595286369\n",
      "Epoch 417: Train loss=0.01436655130237341, Validation loss=0.02622426114976406\n",
      "Epoch 418: Train loss=0.014651799574494362, Validation loss=0.023833483457565308\n",
      "Epoch 419: Train loss=0.012743345461785793, Validation loss=0.026387913152575493\n",
      "Epoch 420: Train loss=0.013989361003041267, Validation loss=0.04681897535920143\n",
      "Epoch 421: Train loss=0.013905823230743408, Validation loss=0.048893190920352936\n",
      "Epoch 422: Train loss=0.015528972260653973, Validation loss=0.01705709658563137\n",
      "Epoch 423: Train loss=0.012308305129408836, Validation loss=0.019336717203259468\n",
      "Epoch 424: Train loss=0.01627352461218834, Validation loss=0.053277429193258286\n",
      "Epoch 425: Train loss=0.013445465825498104, Validation loss=0.06886237859725952\n",
      "Epoch 426: Train loss=0.014518816955387592, Validation loss=0.04676571860909462\n",
      "Epoch 427: Train loss=0.012888981960713863, Validation loss=0.0300081018358469\n",
      "Epoch 428: Train loss=0.013108816929161549, Validation loss=0.021736331284046173\n",
      "Epoch 429: Train loss=0.012914980761706829, Validation loss=0.03066234104335308\n",
      "Epoch 430: Train loss=0.01235438697040081, Validation loss=0.03599356487393379\n",
      "Epoch 431: Train loss=0.01251077838242054, Validation loss=0.01965591311454773\n",
      "Epoch 432: Train loss=0.012893912382423878, Validation loss=0.042689379304647446\n",
      "Epoch 433: Train loss=0.012489390559494495, Validation loss=0.0277478639036417\n",
      "Epoch 434: Train loss=0.013472302816808224, Validation loss=0.03198600560426712\n",
      "Epoch 435: Train loss=0.012780866585671902, Validation loss=0.03573255613446236\n",
      "Epoch 436: Train loss=0.012421922758221626, Validation loss=0.05484498292207718\n",
      "Epoch 437: Train loss=0.013161052949726582, Validation loss=0.035654466599226\n",
      "Epoch 438: Train loss=0.013328938744962215, Validation loss=0.03245842084288597\n",
      "Epoch 439: Train loss=0.013526096940040588, Validation loss=0.027307068929076195\n",
      "Epoch 440: Train loss=0.013678992167115211, Validation loss=0.016905060037970543\n",
      "Epoch 441: Train loss=0.013966281898319721, Validation loss=0.025627532973885536\n",
      "Epoch 442: Train loss=0.013301769271492958, Validation loss=0.02721756510436535\n",
      "Epoch 443: Train loss=0.013187413103878498, Validation loss=0.027193883433938026\n",
      "Epoch 444: Train loss=0.01276868861168623, Validation loss=0.022907691076397896\n",
      "Epoch 445: Train loss=0.012333910912275314, Validation loss=0.05492575839161873\n",
      "Epoch 446: Train loss=0.011897786520421505, Validation loss=0.023898541927337646\n",
      "Epoch 447: Train loss=0.012675595469772816, Validation loss=0.029946831986308098\n",
      "Epoch 448: Train loss=0.013305111788213253, Validation loss=0.021938981488347054\n",
      "Epoch 449: Train loss=0.011962191201746464, Validation loss=0.038964416831731796\n",
      "Epoch 450: Train loss=0.012577401474118233, Validation loss=0.020288893952965736\n",
      "Epoch 451: Train loss=0.012864678166806698, Validation loss=0.019899660721421242\n",
      "Epoch 452: Train loss=0.01227423083037138, Validation loss=0.04853865131735802\n",
      "Epoch 453: Train loss=0.012120517902076244, Validation loss=0.017786405980587006\n",
      "Epoch 454: Train loss=0.01377974171191454, Validation loss=0.01904927007853985\n",
      "Epoch 455: Train loss=0.013682235963642597, Validation loss=0.04339373856782913\n",
      "Epoch 456: Train loss=0.012261650525033474, Validation loss=0.031061796471476555\n",
      "Epoch 457: Train loss=0.011995390057563782, Validation loss=0.021903663873672485\n",
      "Epoch 458: Train loss=0.012061184272170067, Validation loss=0.030793771147727966\n",
      "Epoch 459: Train loss=0.01231300737708807, Validation loss=0.03148529678583145\n",
      "Epoch 460: Train loss=0.014415553770959377, Validation loss=0.018869765102863312\n",
      "Epoch 461: Train loss=0.012172025628387928, Validation loss=0.027262810617685318\n",
      "Epoch 462: Train loss=0.013085340149700642, Validation loss=0.030488604679703712\n",
      "Epoch 463: Train loss=0.012200004421174526, Validation loss=0.026794565841555595\n",
      "Epoch 464: Train loss=0.011604923754930496, Validation loss=0.0186602920293808\n",
      "Epoch 465: Train loss=0.012359355576336384, Validation loss=0.035146720707416534\n",
      "Epoch 466: Train loss=0.011559068225324154, Validation loss=0.03936541825532913\n",
      "Epoch 467: Train loss=0.010871694423258305, Validation loss=0.018623238429427147\n",
      "Epoch 468: Train loss=0.012396618723869324, Validation loss=0.0428411029279232\n",
      "Epoch 469: Train loss=0.013674198649823666, Validation loss=0.02763434126973152\n",
      "Epoch 470: Train loss=0.012461881153285503, Validation loss=0.023365817964076996\n",
      "Epoch 471: Train loss=0.011442651972174644, Validation loss=0.0204295814037323\n",
      "Epoch 472: Train loss=0.010355580598115921, Validation loss=0.024286478757858276\n",
      "Epoch 473: Train loss=0.01252332516014576, Validation loss=0.06613648682832718\n",
      "Epoch 474: Train loss=0.012844967655837536, Validation loss=0.044876448810100555\n",
      "Epoch 475: Train loss=0.013929582200944424, Validation loss=0.01677813194692135\n",
      "Epoch 476: Train loss=0.01668614335358143, Validation loss=0.030274678021669388\n",
      "Epoch 477: Train loss=0.01282206829637289, Validation loss=0.015115641057491302\n",
      "Epoch 478: Train loss=0.012908262200653553, Validation loss=0.02288154885172844\n",
      "Epoch 479: Train loss=0.013331161811947823, Validation loss=0.015994315966963768\n",
      "Epoch 480: Train loss=0.010728784836828709, Validation loss=0.02206425368785858\n",
      "Epoch 481: Train loss=0.013832420110702515, Validation loss=0.02906513772904873\n",
      "Epoch 482: Train loss=0.011793217621743679, Validation loss=0.022348677739501\n",
      "Epoch 483: Train loss=0.012667099013924599, Validation loss=0.02896915376186371\n",
      "Epoch 484: Train loss=0.012014798820018768, Validation loss=0.02584795467555523\n",
      "Epoch 485: Train loss=0.011843220330774784, Validation loss=0.0353681854903698\n",
      "Epoch 486: Train loss=0.012766579166054726, Validation loss=0.05161187797784805\n",
      "Epoch 487: Train loss=0.011070321314036846, Validation loss=0.048091400414705276\n",
      "Epoch 488: Train loss=0.010940480045974255, Validation loss=0.02982422709465027\n",
      "Epoch 489: Train loss=0.011494782753288746, Validation loss=0.01879412680864334\n",
      "Epoch 490: Train loss=0.011417831294238567, Validation loss=0.020138416439294815\n",
      "Epoch 491: Train loss=0.012493018992245197, Validation loss=0.021553847938776016\n",
      "Epoch 492: Train loss=0.010819658637046814, Validation loss=0.029902804642915726\n",
      "Epoch 493: Train loss=0.01244580838829279, Validation loss=0.025199970230460167\n",
      "Epoch 494: Train loss=0.010899839922785759, Validation loss=0.01833902671933174\n",
      "Epoch 495: Train loss=0.013487646356225014, Validation loss=0.02498849481344223\n",
      "Epoch 496: Train loss=0.012602402828633785, Validation loss=0.02480139397084713\n",
      "Epoch 497: Train loss=0.01071479357779026, Validation loss=0.03394262120127678\n",
      "Epoch 498: Train loss=0.011150936596095562, Validation loss=0.05777176097035408\n",
      "Epoch 499: Train loss=0.012521875090897083, Validation loss=0.020227869972586632\n",
      "Epoch 500: Train loss=0.010588048957288265, Validation loss=0.020908450707793236\n",
      "Epoch 501: Train loss=0.011139892973005772, Validation loss=0.04531630873680115\n",
      "Epoch 502: Train loss=0.011051553301513195, Validation loss=0.0429203175008297\n",
      "Epoch 503: Train loss=0.0116967111825943, Validation loss=0.047055356204509735\n",
      "Epoch 504: Train loss=0.013528862968087196, Validation loss=0.03376346826553345\n",
      "Epoch 505: Train loss=0.012200506404042244, Validation loss=0.02205622009932995\n",
      "Epoch 506: Train loss=0.010887031443417072, Validation loss=0.019406313076615334\n",
      "Epoch 507: Train loss=0.009842928498983383, Validation loss=0.014926651492714882\n",
      "Epoch 508: Train loss=0.011001002974808216, Validation loss=0.01527155376970768\n",
      "Epoch 509: Train loss=0.011833958327770233, Validation loss=0.04269268736243248\n",
      "Epoch 510: Train loss=0.011232512071728706, Validation loss=0.02860236167907715\n",
      "Epoch 511: Train loss=0.012674885801970959, Validation loss=0.03070947527885437\n",
      "Epoch 512: Train loss=0.01106641162186861, Validation loss=0.049766864627599716\n",
      "Epoch 513: Train loss=0.012258290313184261, Validation loss=0.021475542336702347\n",
      "Epoch 514: Train loss=0.01088164746761322, Validation loss=0.02063126675784588\n",
      "Epoch 515: Train loss=0.010365581139922142, Validation loss=0.03099440596997738\n",
      "Epoch 516: Train loss=0.012370623648166656, Validation loss=0.017768174409866333\n",
      "Epoch 517: Train loss=0.010727991349995136, Validation loss=0.013489410281181335\n",
      "Epoch 518: Train loss=0.012024273164570332, Validation loss=0.04881635308265686\n",
      "Epoch 519: Train loss=0.011043227277696133, Validation loss=0.0276124719530344\n",
      "Epoch 520: Train loss=0.012713159434497356, Validation loss=0.03693358227610588\n",
      "Epoch 521: Train loss=0.012626457028090954, Validation loss=0.021366102620959282\n",
      "Epoch 522: Train loss=0.01163236703723669, Validation loss=0.022773096337914467\n",
      "Epoch 523: Train loss=0.013157923705875874, Validation loss=0.05456319451332092\n",
      "Epoch 524: Train loss=0.010957867838442326, Validation loss=0.016545549035072327\n",
      "Epoch 525: Train loss=0.011388184502720833, Validation loss=0.02763257920742035\n",
      "Epoch 526: Train loss=0.009932677261531353, Validation loss=0.021552154794335365\n",
      "Epoch 527: Train loss=0.010138900950551033, Validation loss=0.019989052787423134\n",
      "Epoch 528: Train loss=0.010655212216079235, Validation loss=0.030627524480223656\n",
      "Epoch 529: Train loss=0.011044894345104694, Validation loss=0.01730731874704361\n",
      "Epoch 530: Train loss=0.012050523422658443, Validation loss=0.013369805179536343\n",
      "Epoch 531: Train loss=0.010981532745063305, Validation loss=0.019332267343997955\n",
      "Epoch 532: Train loss=0.010433927178382874, Validation loss=0.04238121584057808\n",
      "Epoch 533: Train loss=0.009940615855157375, Validation loss=0.024962468072772026\n",
      "Epoch 534: Train loss=0.012355510145425797, Validation loss=0.021947506815195084\n",
      "Epoch 535: Train loss=0.009439452551305294, Validation loss=0.01890334114432335\n",
      "Epoch 536: Train loss=0.011271114461123943, Validation loss=0.022812336683273315\n",
      "Epoch 537: Train loss=0.010587147437036037, Validation loss=0.03537526726722717\n",
      "Epoch 538: Train loss=0.012118383310735226, Validation loss=0.030690301209688187\n",
      "Epoch 539: Train loss=0.010320007801055908, Validation loss=0.08590639382600784\n",
      "Epoch 540: Train loss=0.01253578718751669, Validation loss=0.0864841565489769\n",
      "Epoch 541: Train loss=0.011775222606956959, Validation loss=0.05200197175145149\n",
      "Epoch 542: Train loss=0.009969053789973259, Validation loss=0.029731009155511856\n",
      "Epoch 543: Train loss=0.01251290924847126, Validation loss=0.015115669928491116\n",
      "Epoch 544: Train loss=0.011167201213538647, Validation loss=0.026594793424010277\n",
      "Epoch 545: Train loss=0.010146339423954487, Validation loss=0.026677653193473816\n",
      "Epoch 546: Train loss=0.01086031086742878, Validation loss=0.04047373682260513\n",
      "Epoch 547: Train loss=0.010052847675979137, Validation loss=0.035967569798231125\n",
      "Epoch 548: Train loss=0.010911613702774048, Validation loss=0.02658114768564701\n",
      "Epoch 549: Train loss=0.012401274405419827, Validation loss=0.03498784452676773\n",
      "Epoch 550: Train loss=0.010764810256659985, Validation loss=0.01282068807631731\n",
      "Epoch 551: Train loss=0.00963888131082058, Validation loss=0.014118291437625885\n",
      "Epoch 552: Train loss=0.009924862533807755, Validation loss=0.03280719369649887\n",
      "Epoch 553: Train loss=0.01004370953887701, Validation loss=0.04531103000044823\n",
      "Epoch 554: Train loss=0.010199476033449173, Validation loss=0.06217223033308983\n",
      "Epoch 555: Train loss=0.011934533715248108, Validation loss=0.020156439393758774\n",
      "Epoch 556: Train loss=0.010782488621771336, Validation loss=0.025505756959319115\n",
      "Epoch 557: Train loss=0.0096885422244668, Validation loss=0.023108594119548798\n",
      "Epoch 558: Train loss=0.012462853454053402, Validation loss=0.014753857627511024\n",
      "Epoch 559: Train loss=0.012061252258718014, Validation loss=0.033964406698942184\n",
      "Epoch 560: Train loss=0.010145599953830242, Validation loss=0.022485610097646713\n",
      "Epoch 561: Train loss=0.010636154562234879, Validation loss=0.03858397156000137\n",
      "Epoch 562: Train loss=0.010372024960815907, Validation loss=0.04613311216235161\n",
      "Epoch 563: Train loss=0.012839548289775848, Validation loss=0.03804413601756096\n",
      "Epoch 564: Train loss=0.011318973265588284, Validation loss=0.020163601264357567\n",
      "Epoch 565: Train loss=0.009440443478524685, Validation loss=0.02858596481382847\n",
      "Epoch 566: Train loss=0.011668786406517029, Validation loss=0.016809586435556412\n",
      "Epoch 567: Train loss=0.009925667196512222, Validation loss=0.047152306884527206\n",
      "Epoch 568: Train loss=0.00954347848892212, Validation loss=0.04655725881457329\n",
      "Epoch 569: Train loss=0.009153957478702068, Validation loss=0.01700408011674881\n",
      "Epoch 570: Train loss=0.009608415886759758, Validation loss=0.022500796243548393\n",
      "Epoch 571: Train loss=0.010006288066506386, Validation loss=0.040621157735586166\n",
      "Epoch 572: Train loss=0.011364701204001904, Validation loss=0.09667538851499557\n",
      "Epoch 573: Train loss=0.012804347090423107, Validation loss=0.030529091134667397\n",
      "Epoch 574: Train loss=0.010955387726426125, Validation loss=0.017520925030112267\n",
      "Epoch 575: Train loss=0.012020309455692768, Validation loss=0.022696366533637047\n",
      "Epoch 576: Train loss=0.011904771439731121, Validation loss=0.03167358413338661\n",
      "Epoch 577: Train loss=0.011613214388489723, Validation loss=0.08544577658176422\n",
      "Epoch 578: Train loss=0.010863208211958408, Validation loss=0.0195439625531435\n",
      "Epoch 579: Train loss=0.010468238964676857, Validation loss=0.014418072998523712\n",
      "Epoch 580: Train loss=0.009026123210787773, Validation loss=0.033595599234104156\n",
      "Epoch 581: Train loss=0.008632413111627102, Validation loss=0.08439366519451141\n",
      "Epoch 582: Train loss=0.010242062620818615, Validation loss=0.050388868898153305\n",
      "Epoch 583: Train loss=0.010050477460026741, Validation loss=0.04981882497668266\n",
      "Epoch 584: Train loss=0.010836969129741192, Validation loss=0.030809486284852028\n",
      "Epoch 585: Train loss=0.010477455332875252, Validation loss=0.014265665784478188\n",
      "Epoch 586: Train loss=0.009559477679431438, Validation loss=0.022970519959926605\n",
      "Epoch 587: Train loss=0.008676256984472275, Validation loss=0.02433055080473423\n",
      "Epoch 588: Train loss=0.009456416592001915, Validation loss=0.023933446034789085\n",
      "Epoch 589: Train loss=0.011337696574628353, Validation loss=0.04648348689079285\n",
      "Epoch 590: Train loss=0.01004732120782137, Validation loss=0.04541950300335884\n",
      "Epoch 591: Train loss=0.010288923047482967, Validation loss=0.020356435328722\n",
      "Epoch 592: Train loss=0.009583290666341782, Validation loss=0.017139215022325516\n",
      "Epoch 593: Train loss=0.009667396545410156, Validation loss=0.022506406530737877\n",
      "Epoch 594: Train loss=0.011597282253205776, Validation loss=0.016990743577480316\n",
      "Epoch 595: Train loss=0.01008050050586462, Validation loss=0.02174033410847187\n",
      "Epoch 596: Train loss=0.010263481177389622, Validation loss=0.03113747015595436\n",
      "Epoch 597: Train loss=0.009931986220180988, Validation loss=0.047878630459308624\n",
      "Epoch 598: Train loss=0.010894627310335636, Validation loss=0.028952840715646744\n",
      "Epoch 599: Train loss=0.00964539684355259, Validation loss=0.07508432120084763\n",
      "Epoch 600: Train loss=0.010487259365618229, Validation loss=0.0176618043333292\n",
      "Epoch 601: Train loss=0.009193923324346542, Validation loss=0.017775913700461388\n",
      "Epoch 602: Train loss=0.010739733465015888, Validation loss=0.020448822528123856\n",
      "Epoch 603: Train loss=0.009323853068053722, Validation loss=0.01930047571659088\n",
      "Epoch 604: Train loss=0.010321050882339478, Validation loss=0.032216619700193405\n",
      "Epoch 605: Train loss=0.009251526556909084, Validation loss=0.03396504372358322\n",
      "Epoch 606: Train loss=0.011035053990781307, Validation loss=0.05850052461028099\n",
      "Epoch 607: Train loss=0.009992613457143307, Validation loss=0.050240278244018555\n",
      "Epoch 608: Train loss=0.008489339612424374, Validation loss=0.017300603911280632\n",
      "Epoch 609: Train loss=0.009240216575562954, Validation loss=0.04613124206662178\n",
      "Epoch 610: Train loss=0.008455825969576836, Validation loss=0.02370164543390274\n",
      "Epoch 611: Train loss=0.009591399691998959, Validation loss=0.02966802753508091\n",
      "Epoch 612: Train loss=0.009125743061304092, Validation loss=0.013168330304324627\n",
      "Epoch 613: Train loss=0.009686182253062725, Validation loss=0.06166669726371765\n",
      "Epoch 614: Train loss=0.008767413906753063, Validation loss=0.019770005717873573\n",
      "Epoch 615: Train loss=0.009970582090318203, Validation loss=0.013670752756297588\n",
      "Epoch 616: Train loss=0.009563266299664974, Validation loss=0.05403539165854454\n",
      "Epoch 617: Train loss=0.010030979290604591, Validation loss=0.015719618648290634\n",
      "Epoch 618: Train loss=0.00906781479716301, Validation loss=0.016107162460684776\n",
      "Epoch 619: Train loss=0.008472749032080173, Validation loss=0.028677700087428093\n",
      "Epoch 620: Train loss=0.009224386885762215, Validation loss=0.024172307923436165\n",
      "Epoch 621: Train loss=0.011021411046385765, Validation loss=0.024087607860565186\n",
      "Epoch 622: Train loss=0.00952017493546009, Validation loss=0.025790175423026085\n",
      "Epoch 623: Train loss=0.011305859312415123, Validation loss=0.05082640051841736\n",
      "Epoch 624: Train loss=0.009935731999576092, Validation loss=0.04137508198618889\n",
      "Epoch 625: Train loss=0.009208937175571918, Validation loss=0.0244424007833004\n",
      "Epoch 626: Train loss=0.009490001015365124, Validation loss=0.054328061640262604\n",
      "Epoch 627: Train loss=0.008312082849442959, Validation loss=0.03148467466235161\n",
      "Epoch 628: Train loss=0.01031822431832552, Validation loss=0.018195584416389465\n",
      "Epoch 629: Train loss=0.009157514199614525, Validation loss=0.01631184108555317\n",
      "Epoch 630: Train loss=0.009299051016569138, Validation loss=0.032797545194625854\n",
      "Epoch 631: Train loss=0.009405677206814289, Validation loss=0.02194380946457386\n",
      "Epoch 632: Train loss=0.008298179134726524, Validation loss=0.016792714595794678\n",
      "Epoch 633: Train loss=0.009839369915425777, Validation loss=0.019515519961714745\n",
      "Epoch 634: Train loss=0.009837694466114044, Validation loss=0.022743944078683853\n",
      "Epoch 635: Train loss=0.010163658298552036, Validation loss=0.019476575776934624\n",
      "Epoch 636: Train loss=0.00994058232754469, Validation loss=0.0182656142860651\n",
      "Epoch 637: Train loss=0.008062812499701977, Validation loss=0.05676420405507088\n",
      "Epoch 638: Train loss=0.009250766597688198, Validation loss=0.04992074891924858\n",
      "Epoch 639: Train loss=0.009280513972043991, Validation loss=0.07298994064331055\n",
      "Epoch 640: Train loss=0.008671846240758896, Validation loss=0.06255485117435455\n",
      "Epoch 641: Train loss=0.00870207604020834, Validation loss=0.040689337998628616\n",
      "Epoch 642: Train loss=0.008613715879619122, Validation loss=0.015486709773540497\n",
      "Epoch 643: Train loss=0.009411641396582127, Validation loss=0.04490780457854271\n",
      "Epoch 644: Train loss=0.009774277918040752, Validation loss=0.028114382177591324\n",
      "Epoch 645: Train loss=0.008363270200788975, Validation loss=0.017879018560051918\n",
      "Epoch 646: Train loss=0.007833071984350681, Validation loss=0.056149113923311234\n",
      "Epoch 647: Train loss=0.008998042903840542, Validation loss=0.0284743495285511\n",
      "Epoch 648: Train loss=0.008062014356255531, Validation loss=0.037859223783016205\n",
      "Epoch 649: Train loss=0.009546108543872833, Validation loss=0.05214685946702957\n",
      "Epoch 650: Train loss=0.008110678754746914, Validation loss=0.05805860832333565\n",
      "Epoch 651: Train loss=0.008169252425432205, Validation loss=0.018558714538812637\n",
      "Epoch 652: Train loss=0.009565073065459728, Validation loss=0.01987011916935444\n",
      "Epoch 653: Train loss=0.009304224513471127, Validation loss=0.04434598237276077\n",
      "Epoch 654: Train loss=0.008632737211883068, Validation loss=0.03948064148426056\n",
      "Epoch 655: Train loss=0.00899961031973362, Validation loss=0.0445038340985775\n",
      "Epoch 656: Train loss=0.01115100085735321, Validation loss=0.02230902947485447\n",
      "Epoch 657: Train loss=0.00818675011396408, Validation loss=0.03602227196097374\n",
      "Epoch 658: Train loss=0.01066423486918211, Validation loss=0.03272142633795738\n",
      "Epoch 659: Train loss=0.010288122110068798, Validation loss=0.038309257477521896\n",
      "Epoch 660: Train loss=0.012089780531823635, Validation loss=0.07155279070138931\n",
      "Epoch 661: Train loss=0.010355222970247269, Validation loss=0.025594590231776237\n",
      "Epoch 662: Train loss=0.008734921924769878, Validation loss=0.017034946009516716\n",
      "Epoch 663: Train loss=0.008569558151066303, Validation loss=0.05408855155110359\n",
      "Epoch 664: Train loss=0.0081767737865448, Validation loss=0.02798420563340187\n",
      "Epoch 665: Train loss=0.007871010340750217, Validation loss=0.05569332465529442\n",
      "Epoch 666: Train loss=0.009569446556270123, Validation loss=0.024005113169550896\n",
      "Epoch 667: Train loss=0.007884674705564976, Validation loss=0.043901409953832626\n",
      "Epoch 668: Train loss=0.007955977693200111, Validation loss=0.03145939111709595\n",
      "Epoch 669: Train loss=0.009176957421004772, Validation loss=0.04974519833922386\n",
      "Epoch 670: Train loss=0.008588174358010292, Validation loss=0.03769877180457115\n",
      "Epoch 671: Train loss=0.008320826105773449, Validation loss=0.05426807329058647\n",
      "Epoch 672: Train loss=0.008646889589726925, Validation loss=0.015210297890007496\n",
      "Epoch 673: Train loss=0.009341183118522167, Validation loss=0.03332727402448654\n",
      "Epoch 674: Train loss=0.007339805364608765, Validation loss=0.08478635549545288\n",
      "Epoch 675: Train loss=0.009034916758537292, Validation loss=0.011921948753297329\n",
      "Epoch 676: Train loss=0.00907309353351593, Validation loss=0.05173704773187637\n",
      "Epoch 677: Train loss=0.009450373239815235, Validation loss=0.07640687376260757\n",
      "Epoch 678: Train loss=0.009724232368171215, Validation loss=0.03902028873562813\n",
      "Epoch 679: Train loss=0.008449133485555649, Validation loss=0.03067021444439888\n",
      "Epoch 680: Train loss=0.010447289794683456, Validation loss=0.031176814809441566\n"
     ]
    }
   ],
   "source": [
    "from StrokeTrajDataset import StrokeTrajDataset\n",
    "from SimBetaVAE import SimBetaVAE\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "resize = torchvision.transforms.Resize((64, 64), antialias=True)\n",
    "s = resize(strokes)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "model = SimBetaVAE()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_test_split = int(0.8 * len(s))\n",
    "from sklearn.utils import shuffle\n",
    "s, t = shuffle(s, trajectories, random_state=0)\n",
    "train = s[:train_test_split], t[:train_test_split]\n",
    "test = s[train_test_split:], t[train_test_split:]\n",
    "\n",
    "train_dataset = StrokeTrajDataset(*train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataset = StrokeTrajDataset(*test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "max_capacity = 5\n",
    "num_epochs = 1000\n",
    "epochs_per_save = 200\n",
    "\n",
    "def validation(model, dataloader, capacity):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    originals = []\n",
    "    decoder_outputs = []\n",
    "    \n",
    "    for (stroke,trajectory) in dataloader:\n",
    "        batch_size = len(stroke)\n",
    "        stroke = stroke.to(device)\n",
    "        trajectory = trajectory.to(device)\n",
    "        args = model(stroke, trajectory)\n",
    "        loss = model.loss(args, capacity)\n",
    "        total_loss += loss * batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            stroke_dec = args[0][0][i].squeeze(0).cpu().detach().numpy()\n",
    "            traj_dec = args[0][1][i].cpu().detach().numpy()\n",
    "            decoder_outputs.append((stroke_dec, traj_dec))\n",
    "            stroke = args[1][0][i].squeeze(0).cpu().detach().numpy()\n",
    "            traj = args[1][1][i].cpu().detach().numpy()\n",
    "            originals.append((stroke, traj))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    model.train()\n",
    "    return avg_loss, decoder_outputs, originals\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    capacity = epoch / num_epochs * max_capacity\n",
    "    total_loss = 0\n",
    "    for (i, (stroke, trajectory)) in enumerate(train_dataloader):\n",
    "        batch_size = len(stroke)\n",
    "        stroke = stroke.to(device)\n",
    "        trajectory = trajectory.to(device)\n",
    "        args = model(stroke, trajectory)\n",
    "        loss = model.loss(args, capacity)\n",
    "        \n",
    "        total_loss += loss * batch_size\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 0 and epoch%epochs_per_save == epochs_per_save-1:\n",
    "            output_dir = f\"training_outputs/epoch{epoch}/train\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for i in range(batch_size):\n",
    "                stroke_dec = args[0][0][i].squeeze(0).cpu().detach().numpy()\n",
    "                traj_dec = args[0][1][i].cpu().detach().numpy()\n",
    "                stroke = args[1][0][i].squeeze(0).cpu().detach().numpy()\n",
    "                traj = args[1][1][i].cpu().detach().numpy()\n",
    "\n",
    "                plt.imsave(f\"{output_dir}/{i}-decoded.png\", stroke_dec)\n",
    "                plt.imsave(f\"{output_dir}/{i}-original.png\", stroke)\n",
    "                visualize_points(traj_dec).save(f\"{output_dir}/{i}-traj_decoded.png\")\n",
    "                visualize_points(traj).save(f\"{output_dir}/{i}-traj_original.png\")\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataset)\n",
    "    val_loss, decoded_list, original_list = validation(model, val_dataloader, capacity)\n",
    "\n",
    "    if epoch%epochs_per_save == epochs_per_save-1:\n",
    "        output_dir = f\"training_outputs/epoch{epoch}/val\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for i in range(len(decoded_list)):\n",
    "            stroke_dec, traj_dec = decoded_list[i][0], decoded_list[i][1]\n",
    "            stroke, traj = original_list[i][0], original_list[i][1]\n",
    "            plt.imsave(f\"{output_dir}/{i}-decoded.png\", stroke_dec)\n",
    "            plt.imsave(f\"{output_dir}/{i}-original.png\", stroke)\n",
    "            visualize_points(traj_dec).save(f\"{output_dir}/{i}-traj_decoded.png\")\n",
    "            visualize_points(traj).save(f\"{output_dir}/{i}-traj_original.png\")\n",
    "    print(f\"Epoch {epoch}: Train loss={train_loss}, Validation loss={val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2ca44-4e69-4da7-adca-5496cc316b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, t = model.sample_latent(4)\n",
    "_, axs = plt.subplots(4, 2)\n",
    "for i in range(4):\n",
    "    axs[i][0].imshow(s[i].squeeze(0).cpu().detach().numpy())\n",
    "    axs[i][1].imshow(visualize_points(t[i].cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db36b36-2c05-4dab-97ea-38bf64716c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_gap_plot(data): # https://stackoverflow.com/questions/42675864/how-to-remove-gaps-between-image-subplots\n",
    "    # data: N x M x individual image\n",
    "    N = len(data)\n",
    "    M = len(data[0])\n",
    "    heights = [50 for a in data]\n",
    "    widths = [50 for a in data[0]]\n",
    "    \n",
    "    fig_width = 8.  # inches\n",
    "    fig_height = fig_width * sum(heights) / sum(widths)\n",
    "\n",
    "    f, axarr = plt.subplots(N,M, figsize=(fig_width, fig_height), gridspec_kw={'height_ratios':heights})\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            axarr[i, j].imshow(data[i][j])\n",
    "            axarr[i, j].axis('off')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e30b7af-cacb-46c4-ba69-a912dc941a98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolating between strokes\n",
    "def interpolate(idx0, idx1):\n",
    "    s = resize(strokes)\n",
    "    t = trajectories\n",
    "    s0 = s[idx0].unsqueeze(0).unsqueeze(0).to(device)\n",
    "    t0 = t[idx0].unsqueeze(0).to(device)\n",
    "    s1 = s[idx1].unsqueeze(0).unsqueeze(0).to(device)\n",
    "    t1 = t[idx1].unsqueeze(0).to(device)\n",
    "    enc0, _ = model.encode(s0, t0)\n",
    "    enc1, _ = model.encode(s1, t1)\n",
    "    \n",
    "    num_images = 5\n",
    "    data = []\n",
    "    for i in range(num_images):\n",
    "        t = i / (num_images - 1)\n",
    "        enc = enc0*(1-t) + enc1*t\n",
    "        stroke, trajectory = model.decode(enc)\n",
    "        stroke = stroke.cpu().detach().reshape((64, 64))\n",
    "        trajectory = visualize_points(trajectory.cpu().detach().squeeze(0))\n",
    "        data.append([stroke, trajectory])\n",
    "    no_gap_plot(data)\n",
    "\n",
    "interpolate(11, 202)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf079c6-0aeb-481e-8a3b-00af367b5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying different latent variables at a time\n",
    "def vary_one_at_a_time(idx):\n",
    "    rs = resize(strokes)\n",
    "    s = rs[idx].unsqueeze(0).unsqueeze(0).to(device)\n",
    "    t = trajectories[idx].unsqueeze(0).to(device)\n",
    "    enc, _ = model.encode(s, t)\n",
    "    s_data = []\n",
    "    t_data = []\n",
    "    for i in range(5):\n",
    "        s_data_row = []\n",
    "        t_data_row = []\n",
    "        for j in range(-3, 4):\n",
    "            shift = torch.zeros(5)\n",
    "            shift[i] += j\n",
    "            shift = shift.to(device)\n",
    "            shifted = enc + shift\n",
    "            stroke, trajectory = model.decode(shifted)\n",
    "            stroke = stroke.cpu().detach().reshape((64, 64))\n",
    "            trajectory = visualize_points(trajectory.cpu().detach().squeeze(0))\n",
    "            s_data_row.append(stroke)\n",
    "            t_data_row.append(trajectory)\n",
    "        s_data.append(s_data_row)\n",
    "        t_data.append(t_data_row)\n",
    "    data = s_data + t_data\n",
    "    no_gap_plot(data)\n",
    "vary_one_at_a_time(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
