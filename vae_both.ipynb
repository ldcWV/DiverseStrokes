{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e54275-f69c-41ca-abe5-963b383bfc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as mc\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895901f5-67a4-4c40-8012-d3c6d6f3c901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ldori\\miniconda3\\envs\\frida\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cache_dir = 'caches/cache_11_14_big'\n",
    "with gzip.GzipFile(os.path.join(cache_dir, \n",
    "        'extended_stroke_library_intensities.npy'),'r') as f:\n",
    "    strokes = np.load(f).astype(np.float32)/255.\n",
    "trajectories = np.load(os.path.join(cache_dir, \n",
    "        'extended_stroke_library_trajectories.npy'), \n",
    "        allow_pickle=True, encoding='bytes') \n",
    "\n",
    "strokes = torch.from_numpy(strokes).float().nan_to_num()\n",
    "trajectories = torch.from_numpy(trajectories.astype(np.float32)).float().nan_to_num()\n",
    "\n",
    "n = len(strokes)\n",
    "\n",
    "# scale_factor = opt.max_height / strokes.shape[1]\n",
    "stroke_shape = np.load(os.path.join(cache_dir, 'stroke_size.npy'))\n",
    "h, w = stroke_shape[0], stroke_shape[1]\n",
    "# strokes = transforms.Resize((int(strokes.shape[1]*scale_factor), int(strokes.shape[2]*scale_factor)))(strokes)\n",
    "strokes = transforms.Resize((h,w))(strokes)\n",
    "\n",
    "hs, he = int(.4*h), int(0.6*h)\n",
    "ws, we = int(0.45*w), int(0.75*w)\n",
    "strokes = strokes[:, hs:he, ws:we]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d9fd646-65b0-4ce8-8dfd-5fb05f45eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_to_points(trajectory, k):\n",
    "    # trajectory: [x0, y0, z0, alpha0, x1, y1, z1, alpha1, ..., x3, y3, z3, alpha3]\n",
    "    # xs and ys represent a cubic bezier curve\n",
    "    # zs are to be interpolated linearly\n",
    "    # alphas are assumed to be constant for now\n",
    "    # returns length k list of [x,y,z,alpha] points tracing the trajectory\n",
    "    \n",
    "    assert k >= 2\n",
    "    \n",
    "    path = np.reshape(trajectory, (4, 4))\n",
    "    \n",
    "    res = []\n",
    "    for i in range(k):\n",
    "        t = i/(k-1)\n",
    "        \n",
    "        # Compute (x,y)\n",
    "        x = (1-t)**3 * path[0][0] \\\n",
    "            + 3*(1-t)**2*t*path[1][0] \\\n",
    "            + 3*(1-t)*t**2*path[2][0] \\\n",
    "            + t**3*path[3][0]\n",
    "        y = (1-t)**3 * path[0][1] \\\n",
    "            + 3*(1-t)**2*t*path[1][1] \\\n",
    "            + 3*(1-t)*t**2*path[2][1] \\\n",
    "            + t**3*path[3][1]\n",
    "        \n",
    "        # Compute z\n",
    "        if t < 0.333:\n",
    "            z = (1 - t/.333) * path[0][2] + (t/.333)*path[1][2]\n",
    "        elif t < 0.666:\n",
    "            z = (1 - (t-.333)/.333) * path[1][2] + ((t-.333)/.333)*path[2][2]\n",
    "        else:\n",
    "            z = (1 - (t-.666)/.333) * path[2][2] + ((t-.666)/.333)*path[3][2]\n",
    "        \n",
    "        # Compute alpha\n",
    "        alpha = path[0][3]\n",
    "        \n",
    "        res.append([x, y, z, alpha])\n",
    "    res = np.array(res)\n",
    "    return res\n",
    "\n",
    "def visualize_points(points):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlim([-0.01, 0.1])\n",
    "    plt.ylim([-0.03, 0.03])\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    lines = []\n",
    "    for i, (x,y,z,alpha) in enumerate(points):\n",
    "        ax.add_patch(plt.Circle((x, y), z/400))\n",
    "        if i != len(points)-1:\n",
    "            lines.append([(x,y),(points[i+1][0], points[i+1][1])])\n",
    "            lc = mc.LineCollection(lines, linewidth=0.1, antialiaseds=False)\n",
    "            ax.add_collection(lc)\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    plt.close()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a50e27-a67d-489f-876c-fd1d8c3733cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([165, 409, 783])\n",
      "(8, 4)\n"
     ]
    }
   ],
   "source": [
    "print(strokes.shape)\n",
    "idx = 9\n",
    "#plt.imshow(strokes[idx])\n",
    "points = trajectory_to_points(trajectories[idx], 8)\n",
    "print(points.shape)\n",
    "png = visualize_points(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31cd429b-4978-4d46-ab3d-55c088ad8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ldori\\AppData\\Local\\Temp\\ipykernel_28592\\1421783745.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  points = torch.Tensor(points)\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "for i in range(len(trajectories)):\n",
    "    points.append(trajectory_to_points(trajectories[i], 8))\n",
    "points = torch.Tensor(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e828c68-7257-401b-8a24-d3bb9d55d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StrokeTrajVAE import StrokeTrajVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cdbf36-64ed-4065-ad9a-153ffc1e65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StrokeTrajVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2a63d3-9205-4808-9895-8974066abd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StrokeTrajDataset import StrokeTrajDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219ab9b7-902c-4fa3-bfe9-ca819938dee6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m stroke \u001b[38;5;241m=\u001b[39m stroke\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     58\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m trajectory\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 59\u001b[0m args \u001b[38;5;241m=\u001b[39m model(stroke, trajectory)\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(args)\n\u001b[0;32m     62\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m batch_size\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\frida\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\DiverseStrokes\\StrokeTrajVAE.py:104\u001b[0m, in \u001b[0;36mStrokeTrajVAE.forward\u001b[1;34m(self, stroke, trajectory)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, stroke, trajectory):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Returns [decoder output, encoder input, log(variance), mean]\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     mean, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(stroke, trajectory)\n\u001b[0;32m    105\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(mean, logvar)\n\u001b[0;32m    106\u001b[0m     stroke_dec, trajectory_dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(latent)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\DiverseStrokes\\StrokeTrajVAE.py:74\u001b[0m, in \u001b[0;36mStrokeTrajVAE.encode\u001b[1;34m(self, stroke, trajectory)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, stroke, trajectory):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# stroke: N x 64 x 64\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# trajectory: N x 8 x 4\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstroke_encoder(stroke)\n\u001b[1;32m---> 74\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x1, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     75\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(trajectory, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraj_encoder(x2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "resize = torchvision.transforms.Resize((64, 64))\n",
    "s = resize(strokes)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "model = StrokeTrajVAE()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_test_split = int(0.8 * len(s))\n",
    "train = s[:train_test_split], points[:train_test_split]\n",
    "test = s[train_test_split:], points[train_test_split:]\n",
    "\n",
    "train_dataset = StrokeTrajDataset(*train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataset = StrokeTrajDataset(*test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "def validation(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    originals = []\n",
    "    decoder_outputs = []\n",
    "    \n",
    "    for (stroke,trajectory) in dataloader:\n",
    "        batch_size = len(stroke)\n",
    "        stroke = stroke.to(device)\n",
    "        trajectory = trajectory.to(device)\n",
    "        args = model(stroke, trajectory)\n",
    "        loss = model.loss(args)\n",
    "        total_loss += loss * batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            stroke_dec = args[0][0][i].squeeze(0).cpu().detach().numpy()\n",
    "            traj_dec = args[0][1][i].cpu().detach().numpy()\n",
    "            decoder_outputs.append((stroke_dec, traj_dec))\n",
    "            stroke = args[1][0][i].squeeze(0).cpu().detach().numpy()\n",
    "            traj = args[1][1][i].cpu().detach().numpy()\n",
    "            originals.append((stroke, traj))\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    model.train()\n",
    "    return avg_loss, decoder_outputs, originals\n",
    "\n",
    "for epoch in range(1500):\n",
    "    total_loss = 0\n",
    "    for (i, (stroke, trajectory)) in enumerate(train_dataloader):\n",
    "        batch_size = len(stroke)\n",
    "        stroke = stroke.to(device)\n",
    "        trajectory = trajectory.to(device)\n",
    "        args = model(stroke, trajectory)\n",
    "        loss = model.loss(args)\n",
    "        \n",
    "        total_loss += loss * batch_size\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i == 0 and epoch%100 == 99:\n",
    "            output_dir = f\"training_outputs/epoch{epoch}/train\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            for i in range(batch_size):\n",
    "                stroke_dec = args[0][0][i].squeeze(0).cpu().detach().numpy()\n",
    "                traj_dec = args[0][1][i].cpu().detach().numpy()\n",
    "                stroke = args[1][0][i].squeeze(0).cpu().detach().numpy()\n",
    "                traj = args[1][1][i].cpu().detach().numpy()\n",
    "\n",
    "                plt.imsave(f\"{output_dir}/{i}-decoded.png\", stroke_dec)\n",
    "                plt.imsave(f\"{output_dir}/{i}-original.png\", stroke)\n",
    "                visualize_points(traj_dec).save(f\"{output_dir}/{i}-traj_decoded.png\")\n",
    "                visualize_points(traj).save(f\"{output_dir}/{i}-traj_original.png\")\n",
    "    \n",
    "    train_loss = total_loss / len(train_dataset)\n",
    "    val_loss, decoded_list, original_list = validation(model, val_dataloader)\n",
    "\n",
    "    if epoch%100 == 99:\n",
    "        output_dir = f\"training_outputs/epoch{epoch}/val\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for i in range(len(decoded_list)):\n",
    "            stroke_dec, traj_dec = decoded_list[i][0], decoded_list[i][1]\n",
    "            stroke, traj = original_list[i][0], original_list[i][1]\n",
    "            plt.imsave(f\"{output_dir}/{i}-decoded.png\", stroke_dec)\n",
    "            plt.imsave(f\"{output_dir}/{i}-original.png\", stroke)\n",
    "            visualize_points(traj_dec).save(f\"{output_dir}/{i}-traj_decoded.png\")\n",
    "            visualize_points(traj).save(f\"{output_dir}/{i}-traj_original.png\")\n",
    "    print(f\"Epoch {epoch}: Train loss={train_loss}, Validation loss={val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa87e2-728f-47e3-8ce6-609a5f7be970",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, t = model.sample_latent(4)\n",
    "_, axs = plt.subplots(4, 2)\n",
    "for i in range(4):\n",
    "    axs[i][0].imshow(s[i].squeeze(0).cpu().detach().numpy())\n",
    "    axs[i][1].imshow(visualize_points(t[i].cpu().detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
